{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 26.232948583420775,
  "eval_steps": 500,
  "global_step": 25000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1049317943336831,
      "grad_norm": 3.3604750633239746,
      "learning_rate": 9.9604e-05,
      "loss": 25.7781,
      "step": 100
    },
    {
      "epoch": 0.2098635886673662,
      "grad_norm": 1.8161104917526245,
      "learning_rate": 9.920400000000001e-05,
      "loss": 3.4314,
      "step": 200
    },
    {
      "epoch": 0.3147953830010493,
      "grad_norm": 0.8619228601455688,
      "learning_rate": 9.880400000000001e-05,
      "loss": 1.1345,
      "step": 300
    },
    {
      "epoch": 0.4197271773347324,
      "grad_norm": 0.3582535982131958,
      "learning_rate": 9.8404e-05,
      "loss": 0.5721,
      "step": 400
    },
    {
      "epoch": 0.5246589716684156,
      "grad_norm": 0.30919337272644043,
      "learning_rate": 9.8004e-05,
      "loss": 0.4559,
      "step": 500
    },
    {
      "epoch": 0.6295907660020986,
      "grad_norm": 0.3635289967060089,
      "learning_rate": 9.760400000000001e-05,
      "loss": 0.3629,
      "step": 600
    },
    {
      "epoch": 0.7345225603357818,
      "grad_norm": 0.2244594693183899,
      "learning_rate": 9.7204e-05,
      "loss": 0.3351,
      "step": 700
    },
    {
      "epoch": 0.8394543546694648,
      "grad_norm": 0.22101451456546783,
      "learning_rate": 9.6804e-05,
      "loss": 0.3224,
      "step": 800
    },
    {
      "epoch": 0.944386149003148,
      "grad_norm": 0.9666449427604675,
      "learning_rate": 9.640400000000001e-05,
      "loss": 0.2878,
      "step": 900
    },
    {
      "epoch": 1.0493179433368311,
      "grad_norm": 0.16317856311798096,
      "learning_rate": 9.600400000000001e-05,
      "loss": 0.2864,
      "step": 1000
    },
    {
      "epoch": 1.154249737670514,
      "grad_norm": 0.1770632266998291,
      "learning_rate": 9.5604e-05,
      "loss": 0.2713,
      "step": 1100
    },
    {
      "epoch": 1.2591815320041972,
      "grad_norm": 0.1899835169315338,
      "learning_rate": 9.5204e-05,
      "loss": 0.2665,
      "step": 1200
    },
    {
      "epoch": 1.3641133263378804,
      "grad_norm": 0.19952958822250366,
      "learning_rate": 9.480400000000001e-05,
      "loss": 0.2682,
      "step": 1300
    },
    {
      "epoch": 1.4690451206715636,
      "grad_norm": 0.10058064013719559,
      "learning_rate": 9.4404e-05,
      "loss": 0.2352,
      "step": 1400
    },
    {
      "epoch": 1.5739769150052467,
      "grad_norm": 0.14895515143871307,
      "learning_rate": 9.4004e-05,
      "loss": 0.258,
      "step": 1500
    },
    {
      "epoch": 1.6789087093389297,
      "grad_norm": 0.1213473454117775,
      "learning_rate": 9.360400000000001e-05,
      "loss": 0.2498,
      "step": 1600
    },
    {
      "epoch": 1.7838405036726128,
      "grad_norm": 0.17849574983119965,
      "learning_rate": 9.3204e-05,
      "loss": 0.2213,
      "step": 1700
    },
    {
      "epoch": 1.888772298006296,
      "grad_norm": 0.0993739664554596,
      "learning_rate": 9.2804e-05,
      "loss": 0.2412,
      "step": 1800
    },
    {
      "epoch": 1.993704092339979,
      "grad_norm": 0.36011508107185364,
      "learning_rate": 9.2404e-05,
      "loss": 0.2045,
      "step": 1900
    },
    {
      "epoch": 2.0986358866736623,
      "grad_norm": 0.15154653787612915,
      "learning_rate": 9.2004e-05,
      "loss": 0.2229,
      "step": 2000
    },
    {
      "epoch": 2.2035676810073452,
      "grad_norm": 0.49846723675727844,
      "learning_rate": 9.1604e-05,
      "loss": 0.2093,
      "step": 2100
    },
    {
      "epoch": 2.308499475341028,
      "grad_norm": 0.13679374754428864,
      "learning_rate": 9.1204e-05,
      "loss": 0.2323,
      "step": 2200
    },
    {
      "epoch": 2.4134312696747116,
      "grad_norm": 0.1131850853562355,
      "learning_rate": 9.080400000000001e-05,
      "loss": 0.2264,
      "step": 2300
    },
    {
      "epoch": 2.5183630640083945,
      "grad_norm": 0.18547628819942474,
      "learning_rate": 9.0404e-05,
      "loss": 0.2272,
      "step": 2400
    },
    {
      "epoch": 2.623294858342078,
      "grad_norm": 0.24238431453704834,
      "learning_rate": 9.0004e-05,
      "loss": 0.2091,
      "step": 2500
    },
    {
      "epoch": 2.728226652675761,
      "grad_norm": 0.22221316397190094,
      "learning_rate": 8.9604e-05,
      "loss": 0.2269,
      "step": 2600
    },
    {
      "epoch": 2.8331584470094437,
      "grad_norm": 0.11537973582744598,
      "learning_rate": 8.920400000000001e-05,
      "loss": 0.2129,
      "step": 2700
    },
    {
      "epoch": 2.938090241343127,
      "grad_norm": 0.1134626716375351,
      "learning_rate": 8.880400000000001e-05,
      "loss": 0.1915,
      "step": 2800
    },
    {
      "epoch": 3.04302203567681,
      "grad_norm": 0.4948520064353943,
      "learning_rate": 8.8404e-05,
      "loss": 0.2287,
      "step": 2900
    },
    {
      "epoch": 3.147953830010493,
      "grad_norm": 0.10147538781166077,
      "learning_rate": 8.8004e-05,
      "loss": 0.1979,
      "step": 3000
    },
    {
      "epoch": 3.2528856243441764,
      "grad_norm": 0.15016181766986847,
      "learning_rate": 8.760400000000001e-05,
      "loss": 0.2247,
      "step": 3100
    },
    {
      "epoch": 3.3578174186778593,
      "grad_norm": 0.14901843667030334,
      "learning_rate": 8.720400000000001e-05,
      "loss": 0.2296,
      "step": 3200
    },
    {
      "epoch": 3.4627492130115423,
      "grad_norm": 0.3421960771083832,
      "learning_rate": 8.6804e-05,
      "loss": 0.2059,
      "step": 3300
    },
    {
      "epoch": 3.5676810073452256,
      "grad_norm": 0.07475756853818893,
      "learning_rate": 8.640400000000001e-05,
      "loss": 0.2022,
      "step": 3400
    },
    {
      "epoch": 3.6726128016789086,
      "grad_norm": 0.16070285439491272,
      "learning_rate": 8.600400000000001e-05,
      "loss": 0.1883,
      "step": 3500
    },
    {
      "epoch": 3.777544596012592,
      "grad_norm": 0.15911494195461273,
      "learning_rate": 8.5604e-05,
      "loss": 0.1854,
      "step": 3600
    },
    {
      "epoch": 3.882476390346275,
      "grad_norm": 0.20954616367816925,
      "learning_rate": 8.5204e-05,
      "loss": 0.205,
      "step": 3700
    },
    {
      "epoch": 3.987408184679958,
      "grad_norm": 0.24099841713905334,
      "learning_rate": 8.480400000000001e-05,
      "loss": 0.1898,
      "step": 3800
    },
    {
      "epoch": 4.092339979013641,
      "grad_norm": 0.10859216749668121,
      "learning_rate": 8.4404e-05,
      "loss": 0.1913,
      "step": 3900
    },
    {
      "epoch": 4.197271773347325,
      "grad_norm": 0.12194084376096725,
      "learning_rate": 8.4004e-05,
      "loss": 0.2017,
      "step": 4000
    },
    {
      "epoch": 4.3022035676810075,
      "grad_norm": 0.11565229296684265,
      "learning_rate": 8.360400000000001e-05,
      "loss": 0.1986,
      "step": 4100
    },
    {
      "epoch": 4.4071353620146905,
      "grad_norm": 0.10394017398357391,
      "learning_rate": 8.3204e-05,
      "loss": 0.1928,
      "step": 4200
    },
    {
      "epoch": 4.512067156348373,
      "grad_norm": 0.149458110332489,
      "learning_rate": 8.2804e-05,
      "loss": 0.1977,
      "step": 4300
    },
    {
      "epoch": 4.616998950682056,
      "grad_norm": 0.3143881559371948,
      "learning_rate": 8.2404e-05,
      "loss": 0.1782,
      "step": 4400
    },
    {
      "epoch": 4.72193074501574,
      "grad_norm": 0.059442188590765,
      "learning_rate": 8.200400000000001e-05,
      "loss": 0.1988,
      "step": 4500
    },
    {
      "epoch": 4.826862539349423,
      "grad_norm": 0.23003530502319336,
      "learning_rate": 8.1604e-05,
      "loss": 0.1957,
      "step": 4600
    },
    {
      "epoch": 4.931794333683106,
      "grad_norm": 0.05788072571158409,
      "learning_rate": 8.1204e-05,
      "loss": 0.1945,
      "step": 4700
    },
    {
      "epoch": 5.036726128016789,
      "grad_norm": 0.101622074842453,
      "learning_rate": 8.080400000000001e-05,
      "loss": 0.2076,
      "step": 4800
    },
    {
      "epoch": 5.141657922350472,
      "grad_norm": 0.19676423072814941,
      "learning_rate": 8.0404e-05,
      "loss": 0.1807,
      "step": 4900
    },
    {
      "epoch": 5.246589716684156,
      "grad_norm": 0.14344638586044312,
      "learning_rate": 8.0004e-05,
      "loss": 0.196,
      "step": 5000
    },
    {
      "epoch": 5.351521511017839,
      "grad_norm": 0.3327390253543854,
      "learning_rate": 7.9604e-05,
      "loss": 0.2098,
      "step": 5100
    },
    {
      "epoch": 5.456453305351522,
      "grad_norm": 0.14421896636486053,
      "learning_rate": 7.9204e-05,
      "loss": 0.1938,
      "step": 5200
    },
    {
      "epoch": 5.561385099685205,
      "grad_norm": 0.0791015625,
      "learning_rate": 7.8804e-05,
      "loss": 0.1924,
      "step": 5300
    },
    {
      "epoch": 5.6663168940188875,
      "grad_norm": 0.23998630046844482,
      "learning_rate": 7.8404e-05,
      "loss": 0.1937,
      "step": 5400
    },
    {
      "epoch": 5.771248688352571,
      "grad_norm": 0.12293902039527893,
      "learning_rate": 7.8004e-05,
      "loss": 0.1784,
      "step": 5500
    },
    {
      "epoch": 5.876180482686254,
      "grad_norm": 0.11377664655447006,
      "learning_rate": 7.7604e-05,
      "loss": 0.19,
      "step": 5600
    },
    {
      "epoch": 5.981112277019937,
      "grad_norm": 0.11310528963804245,
      "learning_rate": 7.7204e-05,
      "loss": 0.1797,
      "step": 5700
    },
    {
      "epoch": 6.08604407135362,
      "grad_norm": 0.08254100382328033,
      "learning_rate": 7.680399999999999e-05,
      "loss": 0.2022,
      "step": 5800
    },
    {
      "epoch": 6.190975865687303,
      "grad_norm": 0.23362748324871063,
      "learning_rate": 7.640400000000001e-05,
      "loss": 0.183,
      "step": 5900
    },
    {
      "epoch": 6.295907660020986,
      "grad_norm": 0.17391450703144073,
      "learning_rate": 7.600400000000001e-05,
      "loss": 0.1845,
      "step": 6000
    },
    {
      "epoch": 6.40083945435467,
      "grad_norm": 0.1194772943854332,
      "learning_rate": 7.5604e-05,
      "loss": 0.1954,
      "step": 6100
    },
    {
      "epoch": 6.505771248688353,
      "grad_norm": 0.06005815416574478,
      "learning_rate": 7.520400000000001e-05,
      "loss": 0.1747,
      "step": 6200
    },
    {
      "epoch": 6.610703043022036,
      "grad_norm": 0.12281546741724014,
      "learning_rate": 7.480400000000001e-05,
      "loss": 0.1827,
      "step": 6300
    },
    {
      "epoch": 6.715634837355719,
      "grad_norm": 0.18336200714111328,
      "learning_rate": 7.4404e-05,
      "loss": 0.1842,
      "step": 6400
    },
    {
      "epoch": 6.820566631689402,
      "grad_norm": 0.11432536691427231,
      "learning_rate": 7.4004e-05,
      "loss": 0.1949,
      "step": 6500
    },
    {
      "epoch": 6.9254984260230845,
      "grad_norm": 0.10873208194971085,
      "learning_rate": 7.360400000000001e-05,
      "loss": 0.1832,
      "step": 6600
    },
    {
      "epoch": 7.030430220356768,
      "grad_norm": 0.20682230591773987,
      "learning_rate": 7.320400000000001e-05,
      "loss": 0.1884,
      "step": 6700
    },
    {
      "epoch": 7.135362014690451,
      "grad_norm": 0.2728215456008911,
      "learning_rate": 7.2804e-05,
      "loss": 0.1599,
      "step": 6800
    },
    {
      "epoch": 7.240293809024134,
      "grad_norm": 0.2417616844177246,
      "learning_rate": 7.2404e-05,
      "loss": 0.1753,
      "step": 6900
    },
    {
      "epoch": 7.345225603357817,
      "grad_norm": 0.11071084439754486,
      "learning_rate": 7.200400000000001e-05,
      "loss": 0.1974,
      "step": 7000
    },
    {
      "epoch": 7.4501573976915,
      "grad_norm": 0.10693834722042084,
      "learning_rate": 7.1604e-05,
      "loss": 0.1897,
      "step": 7100
    },
    {
      "epoch": 7.555089192025184,
      "grad_norm": 0.18422150611877441,
      "learning_rate": 7.1204e-05,
      "loss": 0.1717,
      "step": 7200
    },
    {
      "epoch": 7.660020986358867,
      "grad_norm": 0.13276371359825134,
      "learning_rate": 7.080400000000001e-05,
      "loss": 0.1889,
      "step": 7300
    },
    {
      "epoch": 7.76495278069255,
      "grad_norm": 0.0635090172290802,
      "learning_rate": 7.0404e-05,
      "loss": 0.1733,
      "step": 7400
    },
    {
      "epoch": 7.869884575026233,
      "grad_norm": 0.13399474322795868,
      "learning_rate": 7.0004e-05,
      "loss": 0.1848,
      "step": 7500
    },
    {
      "epoch": 7.974816369359916,
      "grad_norm": 0.15161384642124176,
      "learning_rate": 6.9604e-05,
      "loss": 0.2033,
      "step": 7600
    },
    {
      "epoch": 8.0797481636936,
      "grad_norm": 0.16547441482543945,
      "learning_rate": 6.9204e-05,
      "loss": 0.1991,
      "step": 7700
    },
    {
      "epoch": 8.184679958027282,
      "grad_norm": 0.09930631518363953,
      "learning_rate": 6.8804e-05,
      "loss": 0.1832,
      "step": 7800
    },
    {
      "epoch": 8.289611752360965,
      "grad_norm": 0.1040269285440445,
      "learning_rate": 6.8404e-05,
      "loss": 0.1701,
      "step": 7900
    },
    {
      "epoch": 8.39454354669465,
      "grad_norm": 0.10888288915157318,
      "learning_rate": 6.8004e-05,
      "loss": 0.1859,
      "step": 8000
    },
    {
      "epoch": 8.499475341028331,
      "grad_norm": 0.1214253231883049,
      "learning_rate": 6.7604e-05,
      "loss": 0.1999,
      "step": 8100
    },
    {
      "epoch": 8.604407135362015,
      "grad_norm": 0.1054006963968277,
      "learning_rate": 6.7204e-05,
      "loss": 0.1665,
      "step": 8200
    },
    {
      "epoch": 8.709338929695697,
      "grad_norm": 0.09982214868068695,
      "learning_rate": 6.6804e-05,
      "loss": 0.1699,
      "step": 8300
    },
    {
      "epoch": 8.814270724029381,
      "grad_norm": 0.10093295574188232,
      "learning_rate": 6.6404e-05,
      "loss": 0.1695,
      "step": 8400
    },
    {
      "epoch": 8.919202518363065,
      "grad_norm": 0.11988209187984467,
      "learning_rate": 6.6004e-05,
      "loss": 0.1807,
      "step": 8500
    },
    {
      "epoch": 9.024134312696747,
      "grad_norm": 0.21940569579601288,
      "learning_rate": 6.5604e-05,
      "loss": 0.1716,
      "step": 8600
    },
    {
      "epoch": 9.12906610703043,
      "grad_norm": 0.05165955424308777,
      "learning_rate": 6.5204e-05,
      "loss": 0.1796,
      "step": 8700
    },
    {
      "epoch": 9.233997901364113,
      "grad_norm": 0.1652999222278595,
      "learning_rate": 6.4804e-05,
      "loss": 0.185,
      "step": 8800
    },
    {
      "epoch": 9.338929695697797,
      "grad_norm": 0.07155846804380417,
      "learning_rate": 6.4404e-05,
      "loss": 0.1887,
      "step": 8900
    },
    {
      "epoch": 9.44386149003148,
      "grad_norm": 0.13340574502944946,
      "learning_rate": 6.4004e-05,
      "loss": 0.1883,
      "step": 9000
    },
    {
      "epoch": 9.548793284365162,
      "grad_norm": 0.10072393715381622,
      "learning_rate": 6.360400000000001e-05,
      "loss": 0.164,
      "step": 9100
    },
    {
      "epoch": 9.653725078698846,
      "grad_norm": 0.12840186059474945,
      "learning_rate": 6.320400000000001e-05,
      "loss": 0.1706,
      "step": 9200
    },
    {
      "epoch": 9.758656873032528,
      "grad_norm": 0.34347766637802124,
      "learning_rate": 6.2804e-05,
      "loss": 0.1943,
      "step": 9300
    },
    {
      "epoch": 9.863588667366212,
      "grad_norm": 0.30751118063926697,
      "learning_rate": 6.240400000000001e-05,
      "loss": 0.1685,
      "step": 9400
    },
    {
      "epoch": 9.968520461699896,
      "grad_norm": 0.09046897292137146,
      "learning_rate": 6.200400000000001e-05,
      "loss": 0.168,
      "step": 9500
    },
    {
      "epoch": 10.073452256033578,
      "grad_norm": 0.0943514034152031,
      "learning_rate": 6.1604e-05,
      "loss": 0.1862,
      "step": 9600
    },
    {
      "epoch": 10.178384050367262,
      "grad_norm": 0.10762067884206772,
      "learning_rate": 6.1204e-05,
      "loss": 0.1747,
      "step": 9700
    },
    {
      "epoch": 10.283315844700944,
      "grad_norm": 0.08709283173084259,
      "learning_rate": 6.0804e-05,
      "loss": 0.1664,
      "step": 9800
    },
    {
      "epoch": 10.388247639034628,
      "grad_norm": 0.2090178281068802,
      "learning_rate": 6.0404000000000006e-05,
      "loss": 0.1723,
      "step": 9900
    },
    {
      "epoch": 10.493179433368311,
      "grad_norm": 0.06612953543663025,
      "learning_rate": 6.0004000000000004e-05,
      "loss": 0.1815,
      "step": 10000
    },
    {
      "epoch": 10.598111227701994,
      "grad_norm": 0.08081551641225815,
      "learning_rate": 5.960400000000001e-05,
      "loss": 0.1625,
      "step": 10100
    },
    {
      "epoch": 10.703043022035677,
      "grad_norm": 0.16281771659851074,
      "learning_rate": 5.9204000000000004e-05,
      "loss": 0.1971,
      "step": 10200
    },
    {
      "epoch": 10.80797481636936,
      "grad_norm": 0.144611194729805,
      "learning_rate": 5.8804e-05,
      "loss": 0.1776,
      "step": 10300
    },
    {
      "epoch": 10.912906610703043,
      "grad_norm": 0.06842168420553207,
      "learning_rate": 5.8404000000000005e-05,
      "loss": 0.1665,
      "step": 10400
    },
    {
      "epoch": 11.017838405036725,
      "grad_norm": 0.25047025084495544,
      "learning_rate": 5.8004e-05,
      "loss": 0.1726,
      "step": 10500
    },
    {
      "epoch": 11.12277019937041,
      "grad_norm": 0.13854017853736877,
      "learning_rate": 5.7604e-05,
      "loss": 0.1646,
      "step": 10600
    },
    {
      "epoch": 11.227701993704093,
      "grad_norm": 0.16215232014656067,
      "learning_rate": 5.7204e-05,
      "loss": 0.19,
      "step": 10700
    },
    {
      "epoch": 11.332633788037775,
      "grad_norm": 0.11542778462171555,
      "learning_rate": 5.6804e-05,
      "loss": 0.2021,
      "step": 10800
    },
    {
      "epoch": 11.437565582371459,
      "grad_norm": 0.1347150206565857,
      "learning_rate": 5.6404000000000004e-05,
      "loss": 0.1903,
      "step": 10900
    },
    {
      "epoch": 11.54249737670514,
      "grad_norm": 0.20390214025974274,
      "learning_rate": 5.6004e-05,
      "loss": 0.155,
      "step": 11000
    },
    {
      "epoch": 11.647429171038825,
      "grad_norm": 0.14272180199623108,
      "learning_rate": 5.5604e-05,
      "loss": 0.162,
      "step": 11100
    },
    {
      "epoch": 11.752360965372509,
      "grad_norm": 0.07299138605594635,
      "learning_rate": 5.5204e-05,
      "loss": 0.1541,
      "step": 11200
    },
    {
      "epoch": 11.85729275970619,
      "grad_norm": 0.2192542403936386,
      "learning_rate": 5.4804e-05,
      "loss": 0.1682,
      "step": 11300
    },
    {
      "epoch": 11.962224554039874,
      "grad_norm": 0.10153985023498535,
      "learning_rate": 5.4403999999999996e-05,
      "loss": 0.1614,
      "step": 11400
    },
    {
      "epoch": 12.067156348373556,
      "grad_norm": 0.12599577009677887,
      "learning_rate": 5.4004e-05,
      "loss": 0.17,
      "step": 11500
    },
    {
      "epoch": 12.17208814270724,
      "grad_norm": 0.22001689672470093,
      "learning_rate": 5.3604e-05,
      "loss": 0.1777,
      "step": 11600
    },
    {
      "epoch": 12.277019937040924,
      "grad_norm": 0.17181530594825745,
      "learning_rate": 5.3204e-05,
      "loss": 0.1845,
      "step": 11700
    },
    {
      "epoch": 12.381951731374606,
      "grad_norm": 0.16913500428199768,
      "learning_rate": 5.2804e-05,
      "loss": 0.1817,
      "step": 11800
    },
    {
      "epoch": 12.48688352570829,
      "grad_norm": 0.08825182169675827,
      "learning_rate": 5.2403999999999995e-05,
      "loss": 0.1642,
      "step": 11900
    },
    {
      "epoch": 12.591815320041972,
      "grad_norm": 0.13885585963726044,
      "learning_rate": 5.2004e-05,
      "loss": 0.1473,
      "step": 12000
    },
    {
      "epoch": 12.696747114375656,
      "grad_norm": 0.18510977923870087,
      "learning_rate": 5.160400000000001e-05,
      "loss": 0.1802,
      "step": 12100
    },
    {
      "epoch": 12.80167890870934,
      "grad_norm": 0.13272109627723694,
      "learning_rate": 5.1204000000000006e-05,
      "loss": 0.1566,
      "step": 12200
    },
    {
      "epoch": 12.906610703043022,
      "grad_norm": 0.12823325395584106,
      "learning_rate": 5.080400000000001e-05,
      "loss": 0.192,
      "step": 12300
    },
    {
      "epoch": 13.011542497376706,
      "grad_norm": 0.17521609365940094,
      "learning_rate": 5.040400000000001e-05,
      "loss": 0.1729,
      "step": 12400
    },
    {
      "epoch": 13.116474291710388,
      "grad_norm": 0.06799652427434921,
      "learning_rate": 5.0004000000000004e-05,
      "loss": 0.1746,
      "step": 12500
    },
    {
      "epoch": 13.221406086044071,
      "grad_norm": 0.19870230555534363,
      "learning_rate": 4.9604e-05,
      "loss": 0.1647,
      "step": 12600
    },
    {
      "epoch": 13.326337880377755,
      "grad_norm": 0.08874507993459702,
      "learning_rate": 4.9204e-05,
      "loss": 0.1678,
      "step": 12700
    },
    {
      "epoch": 13.431269674711437,
      "grad_norm": 0.0935133621096611,
      "learning_rate": 4.8804e-05,
      "loss": 0.1601,
      "step": 12800
    },
    {
      "epoch": 13.536201469045121,
      "grad_norm": 0.062345683574676514,
      "learning_rate": 4.8404000000000006e-05,
      "loss": 0.1894,
      "step": 12900
    },
    {
      "epoch": 13.641133263378803,
      "grad_norm": 0.08358403295278549,
      "learning_rate": 4.8004e-05,
      "loss": 0.1658,
      "step": 13000
    },
    {
      "epoch": 13.746065057712487,
      "grad_norm": 0.14484703540802002,
      "learning_rate": 4.760400000000001e-05,
      "loss": 0.1838,
      "step": 13100
    },
    {
      "epoch": 13.85099685204617,
      "grad_norm": 0.19588173925876617,
      "learning_rate": 4.7204000000000004e-05,
      "loss": 0.1604,
      "step": 13200
    },
    {
      "epoch": 13.955928646379853,
      "grad_norm": 0.16355721652507782,
      "learning_rate": 4.6804e-05,
      "loss": 0.1587,
      "step": 13300
    },
    {
      "epoch": 14.060860440713537,
      "grad_norm": 0.1481170803308487,
      "learning_rate": 4.6404000000000005e-05,
      "loss": 0.1742,
      "step": 13400
    },
    {
      "epoch": 14.165792235047219,
      "grad_norm": 0.0849342942237854,
      "learning_rate": 4.6004e-05,
      "loss": 0.1792,
      "step": 13500
    },
    {
      "epoch": 14.270724029380903,
      "grad_norm": 0.16220009326934814,
      "learning_rate": 4.5604e-05,
      "loss": 0.1629,
      "step": 13600
    },
    {
      "epoch": 14.375655823714585,
      "grad_norm": 0.18487660586833954,
      "learning_rate": 4.5204e-05,
      "loss": 0.1822,
      "step": 13700
    },
    {
      "epoch": 14.480587618048268,
      "grad_norm": 0.11497437208890915,
      "learning_rate": 4.4804e-05,
      "loss": 0.1851,
      "step": 13800
    },
    {
      "epoch": 14.585519412381952,
      "grad_norm": 0.10426632314920425,
      "learning_rate": 4.4404000000000004e-05,
      "loss": 0.1619,
      "step": 13900
    },
    {
      "epoch": 14.690451206715634,
      "grad_norm": 0.09854473173618317,
      "learning_rate": 4.4004e-05,
      "loss": 0.1637,
      "step": 14000
    },
    {
      "epoch": 14.795383001049318,
      "grad_norm": 0.1177302896976471,
      "learning_rate": 4.3604e-05,
      "loss": 0.1683,
      "step": 14100
    },
    {
      "epoch": 14.900314795383,
      "grad_norm": 0.14870275557041168,
      "learning_rate": 4.3204e-05,
      "loss": 0.1505,
      "step": 14200
    },
    {
      "epoch": 15.005246589716684,
      "grad_norm": 0.1538287103176117,
      "learning_rate": 4.2804e-05,
      "loss": 0.1574,
      "step": 14300
    },
    {
      "epoch": 15.110178384050368,
      "grad_norm": 0.1878741979598999,
      "learning_rate": 4.2404e-05,
      "loss": 0.1716,
      "step": 14400
    },
    {
      "epoch": 15.21511017838405,
      "grad_norm": 0.12822434306144714,
      "learning_rate": 4.2004000000000006e-05,
      "loss": 0.162,
      "step": 14500
    },
    {
      "epoch": 15.320041972717734,
      "grad_norm": 0.1919974684715271,
      "learning_rate": 4.1604000000000004e-05,
      "loss": 0.1898,
      "step": 14600
    },
    {
      "epoch": 15.424973767051416,
      "grad_norm": 0.1700909435749054,
      "learning_rate": 4.1204e-05,
      "loss": 0.1613,
      "step": 14700
    },
    {
      "epoch": 15.5299055613851,
      "grad_norm": 0.0939737930893898,
      "learning_rate": 4.0804000000000004e-05,
      "loss": 0.1815,
      "step": 14800
    },
    {
      "epoch": 15.634837355718783,
      "grad_norm": 0.07386820763349533,
      "learning_rate": 4.0404e-05,
      "loss": 0.1612,
      "step": 14900
    },
    {
      "epoch": 15.739769150052465,
      "grad_norm": 0.10010147839784622,
      "learning_rate": 4.0004000000000005e-05,
      "loss": 0.1789,
      "step": 15000
    },
    {
      "epoch": 15.84470094438615,
      "grad_norm": 0.13643303513526917,
      "learning_rate": 3.9604e-05,
      "loss": 0.1539,
      "step": 15100
    },
    {
      "epoch": 15.949632738719831,
      "grad_norm": 0.11386607587337494,
      "learning_rate": 3.9204e-05,
      "loss": 0.1528,
      "step": 15200
    },
    {
      "epoch": 16.054564533053515,
      "grad_norm": 0.12252990156412125,
      "learning_rate": 3.8804e-05,
      "loss": 0.1536,
      "step": 15300
    },
    {
      "epoch": 16.1594963273872,
      "grad_norm": 0.11930380761623383,
      "learning_rate": 3.8404e-05,
      "loss": 0.148,
      "step": 15400
    },
    {
      "epoch": 16.264428121720883,
      "grad_norm": 0.16926579177379608,
      "learning_rate": 3.8004000000000004e-05,
      "loss": 0.187,
      "step": 15500
    },
    {
      "epoch": 16.369359916054563,
      "grad_norm": 0.11278772354125977,
      "learning_rate": 3.7604e-05,
      "loss": 0.1748,
      "step": 15600
    },
    {
      "epoch": 16.474291710388247,
      "grad_norm": 0.13167917728424072,
      "learning_rate": 3.7204e-05,
      "loss": 0.1637,
      "step": 15700
    },
    {
      "epoch": 16.57922350472193,
      "grad_norm": 0.09563775360584259,
      "learning_rate": 3.6804e-05,
      "loss": 0.1623,
      "step": 15800
    },
    {
      "epoch": 16.684155299055615,
      "grad_norm": 0.11382202804088593,
      "learning_rate": 3.6404e-05,
      "loss": 0.1709,
      "step": 15900
    },
    {
      "epoch": 16.7890870933893,
      "grad_norm": 0.09393547475337982,
      "learning_rate": 3.6004e-05,
      "loss": 0.1829,
      "step": 16000
    },
    {
      "epoch": 16.89401888772298,
      "grad_norm": 0.2196766883134842,
      "learning_rate": 3.560400000000001e-05,
      "loss": 0.1564,
      "step": 16100
    },
    {
      "epoch": 16.998950682056662,
      "grad_norm": 0.0725071057677269,
      "learning_rate": 3.5204000000000004e-05,
      "loss": 0.1615,
      "step": 16200
    },
    {
      "epoch": 17.103882476390346,
      "grad_norm": 0.10527416318655014,
      "learning_rate": 3.4804e-05,
      "loss": 0.1522,
      "step": 16300
    },
    {
      "epoch": 17.20881427072403,
      "grad_norm": 0.11061672866344452,
      "learning_rate": 3.4404000000000005e-05,
      "loss": 0.1702,
      "step": 16400
    },
    {
      "epoch": 17.313746065057714,
      "grad_norm": 0.16383403539657593,
      "learning_rate": 3.4004e-05,
      "loss": 0.1708,
      "step": 16500
    },
    {
      "epoch": 17.418677859391394,
      "grad_norm": 0.23603491485118866,
      "learning_rate": 3.3604e-05,
      "loss": 0.1656,
      "step": 16600
    },
    {
      "epoch": 17.523609653725078,
      "grad_norm": 0.14278872311115265,
      "learning_rate": 3.3204e-05,
      "loss": 0.1751,
      "step": 16700
    },
    {
      "epoch": 17.628541448058762,
      "grad_norm": 0.1105828508734703,
      "learning_rate": 3.2804e-05,
      "loss": 0.1537,
      "step": 16800
    },
    {
      "epoch": 17.733473242392446,
      "grad_norm": 0.15547116100788116,
      "learning_rate": 3.2404000000000003e-05,
      "loss": 0.175,
      "step": 16900
    },
    {
      "epoch": 17.83840503672613,
      "grad_norm": 0.21198353171348572,
      "learning_rate": 3.2004e-05,
      "loss": 0.1583,
      "step": 17000
    },
    {
      "epoch": 17.94333683105981,
      "grad_norm": 0.08844644576311111,
      "learning_rate": 3.1604e-05,
      "loss": 0.1412,
      "step": 17100
    },
    {
      "epoch": 18.048268625393494,
      "grad_norm": 0.14368993043899536,
      "learning_rate": 3.1204e-05,
      "loss": 0.1748,
      "step": 17200
    },
    {
      "epoch": 18.153200419727177,
      "grad_norm": 0.07312421500682831,
      "learning_rate": 3.0804e-05,
      "loss": 0.1892,
      "step": 17300
    },
    {
      "epoch": 18.25813221406086,
      "grad_norm": 0.19279450178146362,
      "learning_rate": 3.0404e-05,
      "loss": 0.1573,
      "step": 17400
    },
    {
      "epoch": 18.363064008394545,
      "grad_norm": 0.36420997977256775,
      "learning_rate": 3.0004e-05,
      "loss": 0.1755,
      "step": 17500
    },
    {
      "epoch": 18.467995802728225,
      "grad_norm": 0.07668159902095795,
      "learning_rate": 2.9604000000000003e-05,
      "loss": 0.1649,
      "step": 17600
    },
    {
      "epoch": 18.57292759706191,
      "grad_norm": 0.13009662926197052,
      "learning_rate": 2.9204000000000004e-05,
      "loss": 0.1587,
      "step": 17700
    },
    {
      "epoch": 18.677859391395593,
      "grad_norm": 0.09204171597957611,
      "learning_rate": 2.8804000000000004e-05,
      "loss": 0.1494,
      "step": 17800
    },
    {
      "epoch": 18.782791185729277,
      "grad_norm": 0.09291686117649078,
      "learning_rate": 2.8404000000000005e-05,
      "loss": 0.1704,
      "step": 17900
    },
    {
      "epoch": 18.88772298006296,
      "grad_norm": 0.10109245032072067,
      "learning_rate": 2.8004e-05,
      "loss": 0.1632,
      "step": 18000
    },
    {
      "epoch": 18.99265477439664,
      "grad_norm": 0.15913906693458557,
      "learning_rate": 2.7604000000000002e-05,
      "loss": 0.1697,
      "step": 18100
    },
    {
      "epoch": 19.097586568730325,
      "grad_norm": 0.12287448346614838,
      "learning_rate": 2.7204000000000002e-05,
      "loss": 0.1662,
      "step": 18200
    },
    {
      "epoch": 19.20251836306401,
      "grad_norm": 0.20200851559638977,
      "learning_rate": 2.6804000000000003e-05,
      "loss": 0.1751,
      "step": 18300
    },
    {
      "epoch": 19.307450157397692,
      "grad_norm": 0.1288520246744156,
      "learning_rate": 2.6404e-05,
      "loss": 0.1654,
      "step": 18400
    },
    {
      "epoch": 19.412381951731376,
      "grad_norm": 0.14825430512428284,
      "learning_rate": 2.6004e-05,
      "loss": 0.1721,
      "step": 18500
    },
    {
      "epoch": 19.517313746065057,
      "grad_norm": 0.13045820593833923,
      "learning_rate": 2.5604e-05,
      "loss": 0.1787,
      "step": 18600
    },
    {
      "epoch": 19.62224554039874,
      "grad_norm": 0.16480763256549835,
      "learning_rate": 2.5204e-05,
      "loss": 0.1598,
      "step": 18700
    },
    {
      "epoch": 19.727177334732424,
      "grad_norm": 0.13473106920719147,
      "learning_rate": 2.4804000000000002e-05,
      "loss": 0.1495,
      "step": 18800
    },
    {
      "epoch": 19.832109129066108,
      "grad_norm": 0.1398884654045105,
      "learning_rate": 2.4404000000000002e-05,
      "loss": 0.1574,
      "step": 18900
    },
    {
      "epoch": 19.937040923399792,
      "grad_norm": 0.17491646111011505,
      "learning_rate": 2.4004000000000003e-05,
      "loss": 0.1582,
      "step": 19000
    },
    {
      "epoch": 20.041972717733472,
      "grad_norm": 0.15874841809272766,
      "learning_rate": 2.3604e-05,
      "loss": 0.162,
      "step": 19100
    },
    {
      "epoch": 20.146904512067156,
      "grad_norm": 0.08764307200908661,
      "learning_rate": 2.3204e-05,
      "loss": 0.163,
      "step": 19200
    },
    {
      "epoch": 20.25183630640084,
      "grad_norm": 0.10585536062717438,
      "learning_rate": 2.2804e-05,
      "loss": 0.1542,
      "step": 19300
    },
    {
      "epoch": 20.356768100734524,
      "grad_norm": 0.20471175014972687,
      "learning_rate": 2.2404e-05,
      "loss": 0.1743,
      "step": 19400
    },
    {
      "epoch": 20.461699895068207,
      "grad_norm": 0.1309601068496704,
      "learning_rate": 2.2004e-05,
      "loss": 0.1483,
      "step": 19500
    },
    {
      "epoch": 20.566631689401888,
      "grad_norm": 0.10776089131832123,
      "learning_rate": 2.1604000000000002e-05,
      "loss": 0.1458,
      "step": 19600
    },
    {
      "epoch": 20.67156348373557,
      "grad_norm": 0.13457411527633667,
      "learning_rate": 2.1204000000000002e-05,
      "loss": 0.1725,
      "step": 19700
    },
    {
      "epoch": 20.776495278069255,
      "grad_norm": 0.14627230167388916,
      "learning_rate": 2.0804000000000003e-05,
      "loss": 0.1717,
      "step": 19800
    },
    {
      "epoch": 20.88142707240294,
      "grad_norm": 0.18519122898578644,
      "learning_rate": 2.0404e-05,
      "loss": 0.1685,
      "step": 19900
    },
    {
      "epoch": 20.986358866736623,
      "grad_norm": 0.1656610071659088,
      "learning_rate": 2.0004e-05,
      "loss": 0.1635,
      "step": 20000
    },
    {
      "epoch": 21.091290661070303,
      "grad_norm": 0.1415751427412033,
      "learning_rate": 1.9604e-05,
      "loss": 0.1616,
      "step": 20100
    },
    {
      "epoch": 21.196222455403987,
      "grad_norm": 0.1831178218126297,
      "learning_rate": 1.9204e-05,
      "loss": 0.159,
      "step": 20200
    },
    {
      "epoch": 21.30115424973767,
      "grad_norm": 0.40507832169532776,
      "learning_rate": 1.8804e-05,
      "loss": 0.1641,
      "step": 20300
    },
    {
      "epoch": 21.406086044071355,
      "grad_norm": 0.23325258493423462,
      "learning_rate": 1.8404000000000002e-05,
      "loss": 0.1686,
      "step": 20400
    },
    {
      "epoch": 21.511017838405035,
      "grad_norm": 0.14771652221679688,
      "learning_rate": 1.8004000000000002e-05,
      "loss": 0.1502,
      "step": 20500
    },
    {
      "epoch": 21.61594963273872,
      "grad_norm": 0.11845847219228745,
      "learning_rate": 1.7604e-05,
      "loss": 0.1492,
      "step": 20600
    },
    {
      "epoch": 21.720881427072403,
      "grad_norm": 0.126113623380661,
      "learning_rate": 1.7204e-05,
      "loss": 0.1602,
      "step": 20700
    },
    {
      "epoch": 21.825813221406086,
      "grad_norm": 0.22443698346614838,
      "learning_rate": 1.6804e-05,
      "loss": 0.1723,
      "step": 20800
    },
    {
      "epoch": 21.93074501573977,
      "grad_norm": 0.1643981784582138,
      "learning_rate": 1.6404e-05,
      "loss": 0.1792,
      "step": 20900
    },
    {
      "epoch": 22.03567681007345,
      "grad_norm": 0.09079127013683319,
      "learning_rate": 1.6003999999999998e-05,
      "loss": 0.1519,
      "step": 21000
    },
    {
      "epoch": 22.140608604407134,
      "grad_norm": 0.12753580510616302,
      "learning_rate": 1.5604000000000002e-05,
      "loss": 0.1583,
      "step": 21100
    },
    {
      "epoch": 22.24554039874082,
      "grad_norm": 0.11246545612812042,
      "learning_rate": 1.5204000000000002e-05,
      "loss": 0.1598,
      "step": 21200
    },
    {
      "epoch": 22.350472193074502,
      "grad_norm": 0.13125459849834442,
      "learning_rate": 1.4804000000000001e-05,
      "loss": 0.1525,
      "step": 21300
    },
    {
      "epoch": 22.455403987408186,
      "grad_norm": 0.09828809648752213,
      "learning_rate": 1.4404000000000001e-05,
      "loss": 0.1307,
      "step": 21400
    },
    {
      "epoch": 22.560335781741866,
      "grad_norm": 0.260333776473999,
      "learning_rate": 1.4004e-05,
      "loss": 0.1819,
      "step": 21500
    },
    {
      "epoch": 22.66526757607555,
      "grad_norm": 0.12337349355220795,
      "learning_rate": 1.3604e-05,
      "loss": 0.1691,
      "step": 21600
    },
    {
      "epoch": 22.770199370409234,
      "grad_norm": 0.12320461124181747,
      "learning_rate": 1.3204e-05,
      "loss": 0.1612,
      "step": 21700
    },
    {
      "epoch": 22.875131164742918,
      "grad_norm": 0.11325246095657349,
      "learning_rate": 1.2804e-05,
      "loss": 0.1756,
      "step": 21800
    },
    {
      "epoch": 22.9800629590766,
      "grad_norm": 0.2160351723432541,
      "learning_rate": 1.2404e-05,
      "loss": 0.1614,
      "step": 21900
    },
    {
      "epoch": 23.08499475341028,
      "grad_norm": 0.28016436100006104,
      "learning_rate": 1.2004e-05,
      "loss": 0.1791,
      "step": 22000
    },
    {
      "epoch": 23.189926547743966,
      "grad_norm": 0.11621644347906113,
      "learning_rate": 1.1604000000000001e-05,
      "loss": 0.1577,
      "step": 22100
    },
    {
      "epoch": 23.29485834207765,
      "grad_norm": 0.1121063232421875,
      "learning_rate": 1.1204e-05,
      "loss": 0.164,
      "step": 22200
    },
    {
      "epoch": 23.399790136411333,
      "grad_norm": 0.0840429812669754,
      "learning_rate": 1.0804e-05,
      "loss": 0.1462,
      "step": 22300
    },
    {
      "epoch": 23.504721930745017,
      "grad_norm": 0.13317567110061646,
      "learning_rate": 1.0404e-05,
      "loss": 0.1694,
      "step": 22400
    },
    {
      "epoch": 23.609653725078697,
      "grad_norm": 0.29114794731140137,
      "learning_rate": 1.0004000000000001e-05,
      "loss": 0.1525,
      "step": 22500
    },
    {
      "epoch": 23.71458551941238,
      "grad_norm": 0.1433284431695938,
      "learning_rate": 9.604e-06,
      "loss": 0.1724,
      "step": 22600
    },
    {
      "epoch": 23.819517313746065,
      "grad_norm": 0.16201041638851166,
      "learning_rate": 9.204e-06,
      "loss": 0.162,
      "step": 22700
    },
    {
      "epoch": 23.92444910807975,
      "grad_norm": 0.1745411455631256,
      "learning_rate": 8.803999999999999e-06,
      "loss": 0.1601,
      "step": 22800
    },
    {
      "epoch": 24.029380902413433,
      "grad_norm": 0.17170993983745575,
      "learning_rate": 8.404000000000001e-06,
      "loss": 0.154,
      "step": 22900
    },
    {
      "epoch": 24.134312696747113,
      "grad_norm": 0.14533674716949463,
      "learning_rate": 8.004e-06,
      "loss": 0.1621,
      "step": 23000
    },
    {
      "epoch": 24.239244491080797,
      "grad_norm": 0.26066896319389343,
      "learning_rate": 7.604e-06,
      "loss": 0.1767,
      "step": 23100
    },
    {
      "epoch": 24.34417628541448,
      "grad_norm": 0.22910121083259583,
      "learning_rate": 7.204000000000001e-06,
      "loss": 0.157,
      "step": 23200
    },
    {
      "epoch": 24.449108079748164,
      "grad_norm": 0.14627568423748016,
      "learning_rate": 6.804e-06,
      "loss": 0.1577,
      "step": 23300
    },
    {
      "epoch": 24.554039874081848,
      "grad_norm": 0.0906473770737648,
      "learning_rate": 6.404e-06,
      "loss": 0.168,
      "step": 23400
    },
    {
      "epoch": 24.65897166841553,
      "grad_norm": 0.2180468589067459,
      "learning_rate": 6.0040000000000005e-06,
      "loss": 0.1549,
      "step": 23500
    },
    {
      "epoch": 24.763903462749212,
      "grad_norm": 0.1626720130443573,
      "learning_rate": 5.604e-06,
      "loss": 0.1466,
      "step": 23600
    },
    {
      "epoch": 24.868835257082896,
      "grad_norm": 0.17276473343372345,
      "learning_rate": 5.2040000000000005e-06,
      "loss": 0.1455,
      "step": 23700
    },
    {
      "epoch": 24.97376705141658,
      "grad_norm": 0.3528757095336914,
      "learning_rate": 4.804e-06,
      "loss": 0.1877,
      "step": 23800
    },
    {
      "epoch": 25.078698845750264,
      "grad_norm": 0.11095096170902252,
      "learning_rate": 4.4040000000000005e-06,
      "loss": 0.1473,
      "step": 23900
    },
    {
      "epoch": 25.183630640083944,
      "grad_norm": 0.1468106061220169,
      "learning_rate": 4.004e-06,
      "loss": 0.1626,
      "step": 24000
    },
    {
      "epoch": 25.288562434417628,
      "grad_norm": 0.21660040318965912,
      "learning_rate": 3.6040000000000006e-06,
      "loss": 0.1669,
      "step": 24100
    },
    {
      "epoch": 25.39349422875131,
      "grad_norm": 0.08131902664899826,
      "learning_rate": 3.204e-06,
      "loss": 0.161,
      "step": 24200
    },
    {
      "epoch": 25.498426023084996,
      "grad_norm": 0.24214830994606018,
      "learning_rate": 2.804e-06,
      "loss": 0.1541,
      "step": 24300
    },
    {
      "epoch": 25.60335781741868,
      "grad_norm": 0.0812888890504837,
      "learning_rate": 2.404e-06,
      "loss": 0.1742,
      "step": 24400
    },
    {
      "epoch": 25.70828961175236,
      "grad_norm": 0.2851143479347229,
      "learning_rate": 2.0039999999999998e-06,
      "loss": 0.1574,
      "step": 24500
    },
    {
      "epoch": 25.813221406086043,
      "grad_norm": 0.160399928689003,
      "learning_rate": 1.604e-06,
      "loss": 0.1698,
      "step": 24600
    },
    {
      "epoch": 25.918153200419727,
      "grad_norm": 0.10334568470716476,
      "learning_rate": 1.204e-06,
      "loss": 0.1541,
      "step": 24700
    },
    {
      "epoch": 26.02308499475341,
      "grad_norm": 0.18595106899738312,
      "learning_rate": 8.04e-07,
      "loss": 0.1487,
      "step": 24800
    },
    {
      "epoch": 26.128016789087095,
      "grad_norm": 0.16969752311706543,
      "learning_rate": 4.04e-07,
      "loss": 0.156,
      "step": 24900
    },
    {
      "epoch": 26.232948583420775,
      "grad_norm": 0.16486221551895142,
      "learning_rate": 4e-09,
      "loss": 0.1672,
      "step": 25000
    }
  ],
  "logging_steps": 100,
  "max_steps": 25000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 27,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7392115968376832e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
